# 决策树学习笔记

> Decision-Tree（一）——  决策树学习笔记主要介绍初识决策树接触到的一些算法基础概念，以及涉及到的一些信息学专业知识，熟悉概念、原理之后总结算法流程，并介绍三种主流的决策树生成算法伪代码，最后的 “决策树剪枝” 部分较为简略，你懂吧，调优一向偷懒的 :D



### 一. 算法简介

**决策树（Decision Tree）**算法是一种机器学习算法，常被用于处理数据挖掘中的分类问题。是一种**有监督学习**，所谓有监督学习就是给定一堆样本，每个样本都有一组特征属性和一个类别，这些类别都是**事先已知**的， 那么通过学习得到一个分类器，这个分类器能够对新出现的对象给出正确的分类。这样的机器学习就被称之为有监督学习。 

决策树可根据使用的意图分为**分类树（目标变量为离散型）**和**回归树（目标变量为连续型）**，本文主要讨论分类决策树。

决策树模型可以分为三个重要部分：**特征选择**、**决策树的生成** 和 **决策树的剪枝**。下面依次来介绍这三个部分。

---

### 二. 算法过程

#### 1. 特征选择

特征选择是指寻找最优的划分属性。每次划分数据集时我们只能选取一个特征属性，通常我们会选取对训练集划分能力（也称分类能力）最强的一个特征属性。 我们希望在不断划分的过程中，决策树的分支节点所包含的样本尽可能属于同一类，即节点的“纯度”越来越高。而选择最优划分特征的标准不同，也导致了决策树算法的不同。 

##### ① 信息增益

为了解释信息增益，需要先介绍**熵**与**条件熵**。在信息论中和概率论中，**熵（Entropy）**表示随机变量不确定性的度量；**条件熵（ Conditional Entropy ）**表示随机变量 $X$ 已知的条件下，随机变量 $Y$ 的不确定性。熵与条件熵之差称为**互信息（ Mutual Information ）**， 决策树中的信息增益等价于训练数据集中类与特征的互信息 。

对于随机变量 $X$，它的熵：

<img src=".\决策树-熵.svg"  />

在随机变量 $X$ 已经给定的条件下，随机变量 $Y$ 的条件熵：

![](.\决策树-条件熵.svg)

特征 $A$ 对训练集 $D$ 的**信息增益**：

![](.\决策树-信息增益.svg)

一般而言， 信息增益越大，则意味着使用特征 $A$ 来对训练集 $D$ 进行划分所获得的 “纯度” 越高。ID3算法就是采用信息增益作为特征选择准则的决策树算法。



##### ② 信息增益率(比)

上面介绍的信息增益值的大小是相对于数据集而言的，并没有绝对意义。 使用信息增益对特征进行选择，**容易偏向于选择取值较多的特征**（比如，如果将每条样本的ID也视为特征属性时，那么其对应的信息增益毫无疑问是所有特征属性中最大的），为此，提出信息增益率（ information gain ratio ）对这一问题进行校正。著名的C4.5决策树算法就是基于信息增益率来进行特征选择的。

特征 $A$ 对训练集 $D$ 的信息增益率 *$g_R(D,A)$* 定义为**其信息增益 $g(D,A)$ **与 **训练集 $D$ 关于特征 $A$ 的经验熵 $H_A(D)$ 之比**^1^：

![](.\决策树-信息增益比.svg)

其中，

![](.\决策树-信息增益比1.svg)

其中分母是 $D$ 集合针对每个划分的熵，而不是每个划分针对类别的熵的加权平均。对于上述划分后分为两个集合的情况，$n$ 取2即可。很明显，划分的集合越少，划分熵即分母就越小，这样信息增益率就越大，这样选择属性就倾向属性值比较少的了。



##### ③ 基尼指数

基尼指数（又称基尼不纯度，Gini impurity），用来度量数据集 $D$ 的不纯度。

假设有 $K$ 个类别，样本点属于第 $k$ 个类的概率为 $p_k$，则基尼指数定义为：

![基尼指数](.\决策树-基尼指数.svg)

特别的，对于数据集 $D$ ，如果根据特征 $A$ 是否取某一可能值 $a$ 可以把 $D$ 分成 $D_1$ 和 $D_2$ 两部分，即

​										$D_1= \lbrace (x,y)∈D|A(x)=a \rbrace，D_2= \lbrace (x,y)∈D|A(x)≠a \rbrace$

则在特征 $A$ 的条件下，$D$ 的基尼指数为： 

![](.\决策树-基尼指数1.svg)

基尼指数表示模型的不纯度，**基尼指数越小则不纯度越低，特征的划分能力越好**，这和信息增益(率)是相反的。CART分类树算法使用基尼系数来代替信息增益率 。



#### 2. 决策树的生成

决策树的生成算法主要有 ID3、C4.5 以及 CART 算法：

| 决策树生成算法 |  划分标准  |
| :------------: | :--------: |
|      ID3       |  信息增益  |
|      C4.5      | 信息增益率 |
|      CART      |  基尼指数  |



下面依次介绍这三种算法

##### ① ID3算法

ID3算法的核心是在决策树各个结点上使用**信息增益**作为特征选择的准则，递归构建决策树。具体方法是：从根结点开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子结点。之后，再对子结点递归地调用以上方法，构建决策树，直到所有特征的信息增益均很小或没有特征可以选择为止，最终得到一个决策树。ID3相当于用极大似然法进行概率模型的选择。

ID3决策树生成算法：

>$输入：训练数据集D，特征集A，阀值 \epsilon$
>$输出：决策树T$
>$1.若D中所有实例属于同一类C_k，则T为单结点树，并将类C_k作为该结点的类标记，返回决策树T$
>$2.若A=\varnothing，则T为单结点树，并将D中实例数最大的类C_k，作为该结点的类标记，返回决策树T$
>$3.否则，计算A中各特征对D的信息增益，选择信息增益率最大的特征A_g$
>$4.如果A_g的信息增益率小于阀值\epsilon，则置T为单结点树，并将D中实例数最大的类C_k作为该结点的类标记，返回决策树T$
>$5.否则，对A_g的每一可能值a_i，依A_g=a_i将D分割为若干非空子集D_i，将D_i中实例数最大的类作为标记，构建子结点，$$由结点及其子结点构成决策树T，返回决策树T$
>$6.对第i个子结点，以D_i为训练集，以A - \{A_g\}为特征集，递归地调用第(1)步~第(5)步，得到子树T_i，并返回子树T_i$



##### ② C4.5算法

针对于信息增益对可取值数目较多的属性有所偏好这一弊端，C4.5算法采用信息增益率来进行特征选择，以减少这一弊端带来的不利影响。需注意的是，信息增益率准则**对可取值数目较少的属性有所偏好**，因此，C4.5算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式的方法选择最优划分属性：**先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的**。

C4.5决策树生成算法：

>$输入：训练数据集D，特征集A，阀值 \epsilon，信息增益阈值\alpha $
>$输出：决策树T$
>$1.若D中所有实例属于同一类C_k，则T为单结点树，并将类C_k作为该结点的类标记，返回决策树T$
>$2.若A=\varnothing，则T为单结点树，并将D中实例数最大的类C_k，作为该结点的类标记，返回决策树T$
>$3.否则，计算A中各特征对D的信息增益和信息增益率，在信息增益大于\alpha的特征中选择信息增益率最大的特征A_g$
>$4.如果A_g的信息增益率小于阀值\epsilon，则置T为单结点树，并将D中实例数最大的类C_k作为该结点的类标记，返回决策树T$
>$5.否则，对A_g的每一可能值a_i，依A_g=a_i将D分割为若干非空子集D_i，将D_i中实例数最大的类作为标记，构建子结点，$$由结点及其子结点构成决策树T，返回决策树T$
>$6.对第i个子结点，以D_i为训练集，以A - \{A_g\}为特征集，递归地调用第(1)步~第(5)步，得到子树T_i，并返回子树T_i$



##### ③ CART

CART（Classification and Regression Tree，分类和回归树），规定决策树是二叉树，只分为“是”和“不是”两种情况。因为ID3算法和C4.5算法采用了较为复杂的熵来度量，所以它们只能处理分类问题。而CART算法既能处理分类问题，又能处理回归问题。 



- **对于分类树，使用基尼指数（Gini Index) **

CART分类树ID3算法和C4.5算法在原理部分差别不大，唯一的区别在于划分属性的原则。CART选择“基尼指数”作为划分属性的选择。 

CARD分类树生成算法：

>$输入：训练数据集D，特征集A，阀值 \epsilon$
>$输出：决策树T$
>$1：设结点的训练数据集为D，计算现有特征对该数据集的基尼指数。此时，对每一个特征A，对其可能取的每个值a，$$根据样本点A=a的测试为“是”或“否”将D分割为D_1和D_2两部分，利用上式Gini(D,A)来计算A=a时的基尼$$指数。$
>$2：在所有可能的特征A以及他们所有可能的切分点a中，选择基尼指数最小的特征及其对应可能的切分点作为最有特征$$与最优切分点。依最优特征与最有切分点，从现结点生成两个子节点，将训练数据集依特征分配到两个子节点中去。$
>$3：对两个子结点递归地调用(1)、(2)，直至满足停止条件。$
>$4：生成CART决策树T$
>$算法停止计算的条件是节点中的样本个数小于预定阈值，或样本集的基尼指数小于预定阈值，或者没有更多特征。$



- **对于回归树，使用平方误差最小**

CARD回归树生成算法^2^：

>偷个懒，贴几篇博客
>
> https://zhuanlan.zhihu.com/p/20794583 
>
> https://zhuanlan.zhihu.com/p/30048822 
>
> https://zhuanlan.zhihu.com/p/39734325



#### 3. 决策树的剪枝

决策树算法中很容易出现 “过拟合” 现象，结果就是导致算法的泛化能力不强，过于针对当前样本集。为了避免这个问题，主要有两种手段：

##### ① 预剪枝

事先设置一个阈值，当熵减小于这个阈值时就停止创建分支。这个方法其实是一种贪心算法，存在一个潜在的问题：决策树创建过程中，每一个分支的确定都会在很大程度上影响最终的结果，所以有可能某一次创建分支时熵并没有显著减小，但这个分支的子分支却有可能使得熵明显减小。因此我们更推荐使用后剪枝策略。

##### ② 后剪枝

先完整地创建决策树，然后尝试消除多余的节点，后剪枝通过**最小化决策树整体的损失函数**来实现。在提高信息增益的基础上，通过对模型的复杂度T施加惩罚，便得到了损失函数的定义： 

![](.\决策树-损失函数.svg)

$\alpha$  的大小反映了对模型训练集拟合度和模型复杂度的折衷考虑。剪枝的过程就是当 $\alpha$ 确定时，选择损失函数最小的模型。 

后剪枝算法：

> $1. 计算每个节点的经验熵；$
>
> $2. 递归地从树的叶节点向上回缩，如果将某一个父节点的所有叶节点合并，能够使得其损失函数减小，则进行剪枝，将父节点$$变成新的叶节点；$
>
> $3. 返回 2，直到不能继续合并。$







下一篇笔记：《Decision-Tree（二）—— 代码实现》



---

*注解*

[1]. 关于信息增益率网上有两种版本，分布不同，我参考的是[其中一种版本]( https://zhuanlan.zhihu.com/p/39734325 )

[2]. CART算法用作回归的情况，[这篇博客](https://zhuanlan.zhihu.com/p/39734325)写的很细致



*引用及参考*

[1]. [决策树]( https://zhuanlan.zhihu.com/p/39734325 )

[2]. [深入理解机器学习——基于决策树的模型（一）：分类树和回归树](https://blog.csdn.net/hy592070616/article/details/81628956)

[3]. [决策树(分类树、回归树）](https://blog.csdn.net/weixin_36586536/article/details/80468426)

[4]. [从决策树到随机森林]( https://zhuanlan.zhihu.com/p/58005769)

[5]. [机器学习之-常见决策树算法(ID3、C4.5、CART)](https://shuwoom.com/?p=1452)

[6]. [决策树（Decision Tree）](https://www.jianshu.com/p/d0a6fabd796c)

[7]. [决策树](https://www.cnblogs.com/xiemaycherry/p/10475067.html)

[8]. [机器学习实战（三）——决策树](https://blog.csdn.net/jiaoyangwm/article/details/79525237#323_ID3C45CART_1115)

[9].《机器学习实战》Peter Herrington